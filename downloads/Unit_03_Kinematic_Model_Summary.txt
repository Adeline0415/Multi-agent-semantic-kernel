1. Robotic Navigation
2. and Exploration
3. Unit 3: Kinematic Model & Path Tracking Control
4. CS, NTHU
5. Outline
6. • Basics of Control System for Automobile
7. • Kinematic Model
8. • Differential Drive
9. • Bicycle Model
10. – Pure Pursuit Control
11. – Stanley Control (Path Coordinate and Control Stabilization)
12. – Linear Quadratic Regulator (LQR)
13. Control Theory: Open Loop Control
14. • Control System: the mechanism that affects the future state of a system
15. • Control Theory: a strategy to change input to desired output
16. (actuating Signal) (Controlled Variable)
17. Input Output
18. Open Loop Control
19. Plant
20. accelerator Speed
21. Control Theory: Close Loop Control
22. Reference Error Input Output
23. Controller Plant
24. Close Loop Control
25. (Feedback Control)
26. (System Output)
27. Sensor
28. (Measured Output)
29. Control Theory : Car Example
30. Controller
31. Desired Speed Input Output
32. (Speed up or
33. Slow down)
34. Speedometer
35. Linear Time Invariant System
36. Laplace transform
37. r e y
38. D G
39. DGr
40. D x G
41. Equivalent
42. Open Loop
43. PID Control : Proportional Gain
44. Speed/ Current
45. 100m
46. Direction Location
47. Reference Error
48. Actuating Controlled
49. 0 m 100 m (desired location) 11
50. 10 m/s
51. 0 m 100 m (desired location) 12
52. 9 m/s
53. 0 m 100 m (desired location) 13
54. (stop at the
55. desired location)
56. distance Speed
57. r(t)
58. 0 m 100 m (desired location) 14
59. time time
60. PID Control : Problem of Proportional Gain
61. Propeller Current
62. Speed Location
63. 100 m
64. 200 rpm
65. Steady
66. state
67. Idea: Consider past information ！
68. 0 m 15
69. PID Control : Integral Gain
70. Overshooting!
71. Consider past information ！
72. Negative
73. Error
74. Integrator
75. t t
76. What if >200 rpm ?
77. Past (Memory)
78. time
79. Reference
80. 0 0 Signal Variable
81. Present
82. PID Control : Differential Gain
83. Signal Variable
84. Derivative
85. 0 t
86. Future
87. Consider Future information ！
88. Produce a measure of the rate of error change.
89. PID Control
90. • Proportional / Integral / Derivative Control
91. Continuous Form : Discrete Form :
92. 𝑡 𝑑𝑒(𝑡)
93. 𝑝 𝑖 𝑡 𝑑 𝑝 𝑖 𝑡 𝑑
94. Path Tracking Problem
95. Goal
96. 𝑃𝑎𝑡ℎ
97. Longitude Control:
98. Consider the distance to the goal.
99. Lateral Control:
100. Consider the error distance to the path.
101. Basic Kinematic Model
102. State: Rotation Matrix:
103. 0 0 1
104. Kinematic Model:
105. ሶ ሶ
106. Left Motor speed Right Motor speed
107. ሶ 𝑅 ሶ
108. 𝜙 𝜙
109. 2 1
110. Differential Drive Vehicle (cont.)
111. Right Wheel: Left Wheel: Kinematic model for differential drive:
112. ሶ 𝑟𝜙ሶ 𝑅
113. 𝑅1 2
114. ሶ −𝑟𝜙
115. 𝑟𝜙 2
116. 1 2𝑙
117. 𝑟𝜙 𝑟𝜙
118. 1 2
119. 𝑙 𝑙 𝑙 𝑙 0 0 1 𝑟𝜙ሶ 𝑟𝜙ሶ
120. 𝑟𝜙ሶ 𝑟𝜙 1ሶ 𝑟𝜙 2ሶ 𝑟𝜙ሶ 2𝑙 2𝑙
121. 2 2
122. 𝑃 𝑃
123. 𝑙 𝑙
124. Differential Drive Vehicle
125. • Given target velocity 𝒗 and angular velocity 𝝎
126. 1 2 𝑟 𝑟
127. 2𝑙 2𝑙
128. ሶ 1 ሶ
129. 2 𝑟 𝑟
130. 𝑟 − 𝜙
131. 1 1
132. 2𝑙 2𝑙 𝑙
133. 𝑟 𝑟
134. Pure Pursuit Control
135. • Concept:
136. – Modify the angular velocity to let the center achieve a point on path
137. 𝑔 𝑔 𝑔
138. 𝐿 𝑅
139. 𝑑 2 𝑑 𝑑
140. 𝑅 𝐿
141. 𝑑 𝑓𝑐 𝑓𝑐 27
142. Kinematic Bicycle Model
143. • Speed and Steering Control
144. instantaneous center of rotation ( ICR)
145. • nonholonomic constraint equations
146. 𝑓 𝑓
147. • Front Wheel Position
148. • Eliminating front wheel position from (1)
149. • Rear wheel satisfied the constrain (2) when
150. • Applying (4)(5) to (3)
151. • Some Property
152. Pure Pursuit Control for Bicycle Model
153. – Control the steer to let the rear wheel achieve a point on the path.
154. 𝑑 𝑓𝑐 𝑓𝑐 34
155. Stanley Control
156. – Exponential stability for front wheel feedback
157. • Differential of error distance
158. 𝑓 𝑒
159. • To achieve exponential stability to path, we can set
160. • It is not defined when |–ke/vf|>1. We can modify the control law to
161. 𝑓 35
162. LQR Control
163. • If we use the motion model with more complex form (e.g. dynamic model), it is hard
164. to directly analyze the error function.
165. • Linear Quadratic Regulator (LQR) introduce the concept of cost function, and try to
166. solve the optimization problem when the motion model is linear form and the cost
167. function is quadratic form.
168. • The formulation of LQR problem:
169. State Error Minimum Control
170. , in which Q is the state weighting matrix and R is the control weighting matrix.
171. 𝑇𝑒𝑟
172. LQR Control (cont.)
173. • The goal is to find the optimal control u* which minimize the total object
174. 𝑢 u
175. • To solve this problem, we first introduce the concept of optimal principle.
176. If we have a optimal control sequence [𝑢 , 𝑢 , 𝑢 , … , 𝑢 ], then the
177. subsequence [𝑢 , 𝑢 , … , 𝑢 ] is also an optimal control sequence.
178. • Follow the concept, we can apply dynamic programming to recursively
179. solve the optimal control from terminal state to current time.
180. • However, we do not know the terminal time or even the terminal time is
181. infinite in most time. In this case, we can solve the LQR using the recursive
182. relation of value function.
183. • Introduce the value function V(x), which is the summing of the future cost.
184. We can write down the recursive form of the discrete time value function:
185. 𝑡 𝑡 𝑡 𝑡
186. 𝑡 𝑡
187. 𝑇 𝑇
188. • Solve the minimum equation
189. • Apply u* to the value function, and get the equation of P
190. Discrete Algebra Riccati Equation (DARE)
191. is the Continuous Algebra Riccati Equation (CARE)
192. • Given discrete Riccati algebra equation
193. equation on until converge :
194. INITIALIZE: 𝑃 ← 𝑄
195. REPEAT
196. 𝜖 ← ||𝑃 − 𝑃||
197. IF 𝜖 < 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 THEN
198. return 𝑃
199. ENDIF
200. 𝑃 ← 𝑃
201. END
202. LQR Control for Kinematic Model
203. • Take an example to solve the LQR optimal control of the kinematic model.
204. • The linear approximate of kinematic motion model:
205. 0 0
206. 𝑒 1 𝑑𝑡 0 0 𝑒 1 𝑑𝑡 0 0 𝑒 𝑃𝑎𝑡ℎ
207. 𝑑 𝑒ሶ 𝑒ሶ 𝑒ሶ
208. ሶ ሶ ሶ
209. 𝐿 𝐿
210. • Solve the DARE to get the P matrix
211. • Finally, we can get the optimal control 𝑒
212. • Following the steps:
213. – Construct the matrix A, B, X of the linear approximation model.
214. 1 𝑑𝑡 0 0
215. A x B u
216. – Solve DARE and get the matrix P of the value function:
217. – Compute the optimal control:
218. Review of Control Algorithms
219. 𝑡 𝑘𝑒
220. 𝑝 𝑖 𝑡 𝑑 𝑓
221. PID Control Stanley Control
222. What’s Next ?
223. Reinforcement Learning
224. Apply the
225. Consider the More complex Don’t need model.
226. kinematic property.
227. Control
228. 𝐃𝐀𝐑𝐄:
229. Q&A